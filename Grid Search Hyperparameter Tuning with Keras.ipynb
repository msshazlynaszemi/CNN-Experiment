{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running on Mac 1.6GHz i5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tune Batch Size and Number of Epochs\n",
    "\n",
    "The batch size in iterative gradient descent is the number of patterns shown to the network before the weights are updated. \n",
    "\n",
    "It is also an optimization in the training of the network, defining how many patterns to read at a time and keep in memory.\n",
    "\n",
    "The number of epochs = is the number of times that the entire training dataset is shown to the network during training. \n",
    "\n",
    "Some networks are sensitive to the batch size, such as LSTM recurrent neural networks and Convolutional Neural Networks.\n",
    "\n",
    "Here we will evaluate a suite of different mini batch sizes from 10 to 100 in steps of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.697917 using {'epochs': 100, 'batch_size': 60}\n",
      "0.585938 (0.030425) with: {'epochs': 10, 'batch_size': 10}\n",
      "0.447917 (0.164011) with: {'epochs': 50, 'batch_size': 10}\n",
      "0.470052 (0.166534) with: {'epochs': 100, 'batch_size': 10}\n",
      "0.466146 (0.086840) with: {'epochs': 10, 'batch_size': 20}\n",
      "0.656250 (0.019137) with: {'epochs': 50, 'batch_size': 20}\n",
      "0.552083 (0.153767) with: {'epochs': 100, 'batch_size': 20}\n",
      "0.453125 (0.084565) with: {'epochs': 10, 'batch_size': 40}\n",
      "0.664062 (0.014616) with: {'epochs': 50, 'batch_size': 40}\n",
      "0.528646 (0.144982) with: {'epochs': 100, 'batch_size': 40}\n",
      "0.555990 (0.121757) with: {'epochs': 10, 'batch_size': 60}\n",
      "0.657552 (0.012890) with: {'epochs': 50, 'batch_size': 60}\n",
      "0.697917 (0.007366) with: {'epochs': 100, 'batch_size': 60}\n",
      "0.415365 (0.128900) with: {'epochs': 10, 'batch_size': 80}\n",
      "0.600260 (0.054345) with: {'epochs': 50, 'batch_size': 80}\n",
      "0.576823 (0.178874) with: {'epochs': 100, 'batch_size': 80}\n",
      "0.572917 (0.064133) with: {'epochs': 10, 'batch_size': 100}\n",
      "0.658854 (0.014382) with: {'epochs': 50, 'batch_size': 100}\n",
      "0.647135 (0.043303) with: {'epochs': 100, 'batch_size': 100}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that the batch size of 60 and 100 epochs achieved the best result of about 69% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Tune the training optimization algorithm\n",
    "\n",
    "Keras offers a suite of different state-of-the-art optimization algorithms.\n",
    "\n",
    "In this example, we tune the optimization algorithm used to train the network, each with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nurshazlynmohdaszemi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/nurshazlynmohdaszemi/anaconda3/envs/tensorflow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.691406 using {'optimizer': 'Adam'}\n",
      "0.651042 (0.024774) with: {'optimizer': 'SGD'}\n",
      "0.664063 (0.016877) with: {'optimizer': 'RMSprop'}\n",
      "0.661458 (0.018136) with: {'optimizer': 'Adagrad'}\n",
      "0.606771 (0.088004) with: {'optimizer': 'Adadelta'}\n",
      "0.691406 (0.009568) with: {'optimizer': 'Adam'}\n",
      "0.690104 (0.012075) with: {'optimizer': 'Adamax'}\n",
      "0.678385 (0.019225) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The results suggest that the ADAM optimization algorithm is the best with a score of about 69% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tune Learning Rate and Momentum\n",
    "\n",
    "common optimization algorithm is Stocastic Gradient Descent (SGD) because it is well understood.\n",
    "\n",
    "Learning rate controls how much to update the weight at the end of each batch and the momentum controls how much to let the previous update influence the current weight update.\n",
    "\n",
    "We will try a suite of small standard learning rates and a momentum values from 0.2 to 0.8 in steps of 0.2, as well as 0.9 (because it can be a popular value in practice).\n",
    "\n",
    "Generally, it is a good idea to also include the number of epochs in an optimization like this as there is a dependency between the amount of learning per batch (learning rate), the number of updates per epoch (batch size) and the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nurshazlynmohdaszemi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/nurshazlynmohdaszemi/anaconda3/envs/tensorflow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.696615 using {'learn_rate': 0.001, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.001, 'momentum': 0.0}\n",
      "0.653646 (0.027498) with: {'learn_rate': 0.001, 'momentum': 0.2}\n",
      "0.648438 (0.022326) with: {'learn_rate': 0.001, 'momentum': 0.4}\n",
      "0.670573 (0.011201) with: {'learn_rate': 0.001, 'momentum': 0.6}\n",
      "0.696615 (0.015073) with: {'learn_rate': 0.001, 'momentum': 0.8}\n",
      "0.533854 (0.149269) with: {'learn_rate': 0.001, 'momentum': 0.9}\n",
      "0.533854 (0.149269) with: {'learn_rate': 0.01, 'momentum': 0.0}\n",
      "0.533854 (0.149269) with: {'learn_rate': 0.01, 'momentum': 0.2}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.01, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.01, 'momentum': 0.6}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.01, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.01, 'momentum': 0.9}\n",
      "0.348958 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.0}\n",
      "0.572917 (0.134575) with: {'learn_rate': 0.1, 'momentum': 0.2}\n",
      "0.533854 (0.149269) with: {'learn_rate': 0.1, 'momentum': 0.4}\n",
      "0.348958 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.6}\n",
      "0.572917 (0.134575) with: {'learn_rate': 0.1, 'momentum': 0.8}\n",
      "0.348958 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.9}\n",
      "0.544271 (0.146518) with: {'learn_rate': 0.2, 'momentum': 0.0}\n",
      "0.652344 (0.022999) with: {'learn_rate': 0.2, 'momentum': 0.2}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.6}\n",
      "0.649740 (0.026557) with: {'learn_rate': 0.2, 'momentum': 0.8}\n",
      "0.572917 (0.134575) with: {'learn_rate': 0.2, 'momentum': 0.9}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.0}\n",
      "0.427083 (0.134575) with: {'learn_rate': 0.3, 'momentum': 0.2}\n",
      "0.466146 (0.149269) with: {'learn_rate': 0.3, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.6}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.8}\n",
      "0.427083 (0.134575) with: {'learn_rate': 0.3, 'momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the learning rate and momentum\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that relatively SGD is not very good on this problem, nevertheless best results were achieved using a learning rate of 0.001 and a momentum of 0.8 with an accuracy of about 69%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tune Network Weight Initialization\n",
    "\n",
    "We will look at tuning the selection of network weight initialization by evaluating all of the available techniques.\n",
    "\n",
    "We will use the same weight initialization method on each layer. Ideally, it may be better to use different weight initialization schemes according to the activation function used on each layer. \n",
    "\n",
    "In the example below we use rectifier for the hidden layer. We use sigmoid for the output layer because the predictions are binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nurshazlynmohdaszemi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/nurshazlynmohdaszemi/anaconda3/envs/tensorflow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.727865 using {'init_mode': 'normal'}\n",
      "0.714844 (0.019918) with: {'init_mode': 'uniform'}\n",
      "0.692708 (0.015073) with: {'init_mode': 'lecun_uniform'}\n",
      "0.727865 (0.009744) with: {'init_mode': 'normal'}\n",
      "0.651042 (0.024774) with: {'init_mode': 'zero'}\n",
      "0.710938 (0.005524) with: {'init_mode': 'glorot_normal'}\n",
      "0.703125 (0.011049) with: {'init_mode': 'glorot_uniform'}\n",
      "0.466146 (0.149269) with: {'init_mode': 'he_normal'}\n",
      "0.660156 (0.012758) with: {'init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the weight initialization\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(init_mode='uniform'):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=8, kernel_initializer=init_mode, activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that the best results were achieved with a normal weight initialization scheme achieving a performance of about 72%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How to Tune Dropout Regularization\n",
    "\n",
    "we will look at tuning the dropout rate for regularization in an effort to limit overfitting and improve the model’s ability to generalize.\n",
    "\n",
    "To get good results, dropout is best combined with a weight constraint such as the max norm constraint.\n",
    "\n",
    "This involves fitting both the dropout percentage and the weight constraint. We will try dropout percentages between 0.0 and 0.9 (1.0 does not make sense) and maxnorm weight constraint values between 0 and 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nurshazlynmohdaszemi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/nurshazlynmohdaszemi/anaconda3/envs/tensorflow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.726563 using {'dropout_rate': 0.3, 'weight_constraint': 3}\n",
      "0.717448 (0.009744) with: {'dropout_rate': 0.0, 'weight_constraint': 1}\n",
      "0.707031 (0.006379) with: {'dropout_rate': 0.0, 'weight_constraint': 2}\n",
      "0.712240 (0.025780) with: {'dropout_rate': 0.0, 'weight_constraint': 3}\n",
      "0.717448 (0.020505) with: {'dropout_rate': 0.0, 'weight_constraint': 4}\n",
      "0.722656 (0.024080) with: {'dropout_rate': 0.0, 'weight_constraint': 5}\n",
      "0.720052 (0.026748) with: {'dropout_rate': 0.1, 'weight_constraint': 1}\n",
      "0.707031 (0.008438) with: {'dropout_rate': 0.1, 'weight_constraint': 2}\n",
      "0.718750 (0.016877) with: {'dropout_rate': 0.1, 'weight_constraint': 3}\n",
      "0.722656 (0.020915) with: {'dropout_rate': 0.1, 'weight_constraint': 4}\n",
      "0.716146 (0.007366) with: {'dropout_rate': 0.1, 'weight_constraint': 5}\n",
      "0.700521 (0.020505) with: {'dropout_rate': 0.2, 'weight_constraint': 1}\n",
      "0.722656 (0.025315) with: {'dropout_rate': 0.2, 'weight_constraint': 2}\n",
      "0.718750 (0.019401) with: {'dropout_rate': 0.2, 'weight_constraint': 3}\n",
      "0.712240 (0.015073) with: {'dropout_rate': 0.2, 'weight_constraint': 4}\n",
      "0.709635 (0.009744) with: {'dropout_rate': 0.2, 'weight_constraint': 5}\n",
      "0.710938 (0.016877) with: {'dropout_rate': 0.3, 'weight_constraint': 1}\n",
      "0.720052 (0.030314) with: {'dropout_rate': 0.3, 'weight_constraint': 2}\n",
      "0.726563 (0.015947) with: {'dropout_rate': 0.3, 'weight_constraint': 3}\n",
      "0.704427 (0.010253) with: {'dropout_rate': 0.3, 'weight_constraint': 4}\n",
      "0.708333 (0.020505) with: {'dropout_rate': 0.3, 'weight_constraint': 5}\n",
      "0.703125 (0.009568) with: {'dropout_rate': 0.4, 'weight_constraint': 1}\n",
      "0.718750 (0.024910) with: {'dropout_rate': 0.4, 'weight_constraint': 2}\n",
      "0.700521 (0.017566) with: {'dropout_rate': 0.4, 'weight_constraint': 3}\n",
      "0.692708 (0.008027) with: {'dropout_rate': 0.4, 'weight_constraint': 4}\n",
      "0.707031 (0.003189) with: {'dropout_rate': 0.4, 'weight_constraint': 5}\n",
      "0.714844 (0.036225) with: {'dropout_rate': 0.5, 'weight_constraint': 1}\n",
      "0.707031 (0.008438) with: {'dropout_rate': 0.5, 'weight_constraint': 2}\n",
      "0.709635 (0.003683) with: {'dropout_rate': 0.5, 'weight_constraint': 3}\n",
      "0.707031 (0.013902) with: {'dropout_rate': 0.5, 'weight_constraint': 4}\n",
      "0.704427 (0.015733) with: {'dropout_rate': 0.5, 'weight_constraint': 5}\n",
      "0.704427 (0.004872) with: {'dropout_rate': 0.6, 'weight_constraint': 1}\n",
      "0.705729 (0.009744) with: {'dropout_rate': 0.6, 'weight_constraint': 2}\n",
      "0.704427 (0.009744) with: {'dropout_rate': 0.6, 'weight_constraint': 3}\n",
      "0.695313 (0.011049) with: {'dropout_rate': 0.6, 'weight_constraint': 4}\n",
      "0.707031 (0.016573) with: {'dropout_rate': 0.6, 'weight_constraint': 5}\n",
      "0.699219 (0.019401) with: {'dropout_rate': 0.7, 'weight_constraint': 1}\n",
      "0.701823 (0.009744) with: {'dropout_rate': 0.7, 'weight_constraint': 2}\n",
      "0.705729 (0.039879) with: {'dropout_rate': 0.7, 'weight_constraint': 3}\n",
      "0.714844 (0.020915) with: {'dropout_rate': 0.7, 'weight_constraint': 4}\n",
      "0.684896 (0.014731) with: {'dropout_rate': 0.7, 'weight_constraint': 5}\n",
      "0.682292 (0.021236) with: {'dropout_rate': 0.8, 'weight_constraint': 1}\n",
      "0.688802 (0.009744) with: {'dropout_rate': 0.8, 'weight_constraint': 2}\n",
      "0.684896 (0.019225) with: {'dropout_rate': 0.8, 'weight_constraint': 3}\n",
      "0.692708 (0.014731) with: {'dropout_rate': 0.8, 'weight_constraint': 4}\n",
      "0.701823 (0.015733) with: {'dropout_rate': 0.8, 'weight_constraint': 5}\n",
      "0.675781 (0.019918) with: {'dropout_rate': 0.9, 'weight_constraint': 1}\n",
      "0.674479 (0.021236) with: {'dropout_rate': 0.9, 'weight_constraint': 2}\n",
      "0.688802 (0.020505) with: {'dropout_rate': 0.9, 'weight_constraint': 3}\n",
      "0.674479 (0.013279) with: {'dropout_rate': 0.9, 'weight_constraint': 4}\n",
      "0.667969 (0.033299) with: {'dropout_rate': 0.9, 'weight_constraint': 5}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the dropout rate\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(dropout_rate=0.0, weight_constraint=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that the dropout rate of 0.3% and the maxnorm weight constraint of 3 resulted in the best accuracy of about 72%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Tune the Number of Neurons in the Hidden Layer\n",
    " \n",
    "The number of neurons in a layer is an important parameter to tune. Generally the number of neurons in a layer controls the representational capacity of the network, at least at that point in the topology.\n",
    "\n",
    "Also, generally, a large enough single layer network can approximate any other neural network, at least in theory.\n",
    "\n",
    " we will look at tuning the number of neurons in a single hidden layer. We will try values from 1 to 30 in steps of 5.\n",
    "\n",
    "A larger network requires more training and at least the batch size and number of epochs should ideally be optimized with the number of neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nurshazlynmohdaszemi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/nurshazlynmohdaszemi/anaconda3/envs/tensorflow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.722656 using {'neurons': 25}\n",
      "0.701823 (0.012890) with: {'neurons': 1}\n",
      "0.712240 (0.009744) with: {'neurons': 5}\n",
      "0.707031 (0.025315) with: {'neurons': 10}\n",
      "0.712240 (0.021236) with: {'neurons': 15}\n",
      "0.717448 (0.025780) with: {'neurons': 20}\n",
      "0.722656 (0.022097) with: {'neurons': 25}\n",
      "0.721354 (0.020505) with: {'neurons': 30}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the number of neurons\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(neurons=1):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(neurons, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(4)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that the best results were achieved with a network with 25 neurons in the hidden layer with an accuracy of about 72%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Tune the Neuron Activation Function\n",
    "\n",
    "The activation function controls the non-linearity of individual neurons and when to fire.\n",
    "\n",
    "Generally, the rectifier activation function is the most popular, but it used to be the sigmoid and the tanh functions and these functions may still be more suitable for different problems.\n",
    "\n",
    "In this example, we will evaluate the suite of different activation functions available in Keras. We will only use these functions in the hidden layer, as we require a sigmoid activation function in the output for the binary classification problem.\n",
    "\n",
    "Generally, it is a good idea to prepare data to the range of the different transfer functions, which we will not do in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nurshazlynmohdaszemi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/nurshazlynmohdaszemi/anaconda3/envs/tensorflow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.718750 using {'activation': 'softplus'}\n",
      "0.669271 (0.006639) with: {'activation': 'softmax'}\n",
      "0.718750 (0.027251) with: {'activation': 'softplus'}\n",
      "0.673177 (0.031304) with: {'activation': 'softsign'}\n",
      "0.714844 (0.027621) with: {'activation': 'relu'}\n",
      "0.667969 (0.015947) with: {'activation': 'tanh'}\n",
      "0.691406 (0.014616) with: {'activation': 'sigmoid'}\n",
      "0.671875 (0.027621) with: {'activation': 'hard_sigmoid'}\n",
      "0.705729 (0.001841) with: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the activation function\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(activation='relu'):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation=activation))\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surprisingly (to me at least), the ‘softplus’ activation function achieved the best results with an accuracy of about 72%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
